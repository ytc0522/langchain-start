{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "为了方便起见，我们将所有必要的RAG代码步骤整合到一个代码单元中：\n",
    "\n"
   ],
   "id": "8e584e1a20e7033a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T03:54:26.152117Z",
     "start_time": "2024-06-07T03:54:21.409515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T03:54:29.834582Z",
     "start_time": "2024-06-07T03:54:26.153230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ],
   "id": "ebba550ffc747937",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:langchain_core.tracers.core:Parent run f540e425-4df6-4e7b-837c-ef73e2bf985e not found for run 60a81824-0d93-41b6-a95b-580154f990ff. Treating as a root run.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Task decomposition involves breaking down complex tasks into smaller and simpler steps to make them more manageable. This process can be achieved through techniques like Chain of Thought (CoT) or Tree of Thoughts, which help agents plan and execute tasks effectively by dividing them into sequential subgoals. Task decomposition can be facilitated by using prompting techniques, task-specific instructions, or human inputs to guide the agent through the steps required to accomplish a task.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:00:38.263744Z",
     "start_time": "2024-06-07T04:00:38.260437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]"
   ],
   "id": "9be2bd3d78ef373a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:00:41.577464Z",
     "start_time": "2024-06-07T04:00:39.846424Z"
    }
   },
   "cell_type": "code",
   "source": "tool.invoke(\"task decomposition\")",
   "id": "bd67e42de284fa28",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "代理\n",
   "id": "4d39d2919157b027"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:37:44.734942Z",
     "start_time": "2024-06-07T04:37:42.489706Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install langgraph",
   "id": "92a99196ed79f73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langgraph\r\n",
      "  Downloading langgraph-0.0.64-py3-none-any.whl.metadata (23 kB)\r\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.2 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langgraph) (0.2.5)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\r\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.66 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.75)\r\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (23.2)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (2.7.3)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langchain-core<0.3,>=0.2->langgraph) (8.3.0)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (2.4)\r\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.10.3)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (2.32.3)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.18.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.12.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (2.2.1)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jack/miniconda3/envs/langchain/lib/python3.11/site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.66->langchain-core<0.3,>=0.2->langgraph) (2024.6.2)\r\n",
      "Downloading langgraph-0.0.64-py3-none-any.whl (88 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.1/88.1 kB\u001B[0m \u001B[31m1.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0meta \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: langgraph\r\n",
      "Successfully installed langgraph-0.0.64\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:39:43.187192Z",
     "start_time": "2024-06-07T04:39:43.157746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools)"
   ],
   "id": "d1b7b3fc3528746a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:40:07.743748Z",
     "start_time": "2024-06-07T04:40:01.031843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]},\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ],
   "id": "8715cf0f113b7fcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Vy92TVvoJKVKDgFiDDOoKb9z', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 68, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bfcb6e6f-49f7-4661-9eef-e1c55c48dd76-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_Vy92TVvoJKVKDgFiDDOoKb9z'}], usage_metadata={'input_tokens': 68, 'output_tokens': 19, 'total_tokens': 87})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', tool_call_id='call_Vy92TVvoJKVKDgFiDDOoKb9z')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. This approach helps in transforming big tasks into multiple manageable tasks, making it easier for agents to plan and execute them effectively. One common method for task decomposition is the Chain of Thought (CoT) technique, where models are instructed to think step by step to decompose hard tasks. Another extension of CoT is the Tree of Thoughts, which explores multiple reasoning possibilities at each step by creating a tree structure of thought steps.\\n\\nTask decomposition can be achieved through various methods, such as using simple prompting for Language Model (LLM) like \"Steps for XYZ\" or task-specific instructions. Additionally, human inputs can also be used for task decomposition. Overall, task decomposition plays a crucial role in planning and executing complex tasks efficiently.', response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 588, 'total_tokens': 750}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-4187b0d7-64ac-4ba6-b413-1368eb27e9a8-0', usage_metadata={'input_tokens': 588, 'output_tokens': 162, 'total_tokens': 750})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:42:07.642069Z",
     "start_time": "2024-06-07T04:42:07.638005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ],
   "id": "8d0ad2ba972dbfa7",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "请注意，如果我们输入不需要检索步骤的查询，则代理不会执行该查询：",
   "id": "1fe933223f810c39"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:42:27.113637Z",
     "start_time": "2024-06-07T04:42:24.844930Z"
    }
   },
   "cell_type": "code",
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Hi! I'm bob\")]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ],
   "id": "167ca60bc7c9009b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='Hello Bob! How can I assist you today?', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 67, 'total_tokens': 78}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-0465bfc2-d90a-407b-bc46-371869363554-0', usage_metadata={'input_tokens': 67, 'output_tokens': 11, 'total_tokens': 78})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "此外，如果我们输入确实需要检索步骤的查询，代理将生成工具的输入：\n",
    "\n"
   ],
   "id": "b7c65fcfab77690f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T04:43:55.325298Z",
     "start_time": "2024-06-07T04:43:49.434378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"What is Task Decomposition?\"\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ],
   "id": "b2e8577ee9692dd0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Epcqfh9Rmg1S0Jj28HfrakMH', 'function': {'arguments': '{\"query\":\"Task Decomposition\"}', 'name': 'blog_post_retriever'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 91, 'total_tokens': 110}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-46537fca-d043-4a28-9220-2dd6b4bf1799-0', tool_calls=[{'name': 'blog_post_retriever', 'args': {'query': 'Task Decomposition'}, 'id': 'call_Epcqfh9Rmg1S0Jj28HfrakMH'}], usage_metadata={'input_tokens': 91, 'output_tokens': 19, 'total_tokens': 110})]}}\n",
      "----\n",
      "{'tools': {'messages': [ToolMessage(content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\\n\\n(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user\\'s request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\\n\\nFig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', name='blog_post_retriever', tool_call_id='call_Epcqfh9Rmg1S0Jj28HfrakMH')]}}\n",
      "----\n",
      "{'agent': {'messages': [AIMessage(content='Task decomposition is a technique used in complex tasks where the task is broken down into smaller and simpler steps. This approach helps in transforming big tasks into multiple manageable tasks, making it easier for an agent to plan and execute the task effectively. By decomposing the problem into smaller steps, the agent can better understand the task and achieve successful outcomes.', response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 611, 'total_tokens': 680}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-70fce690-25ed-400e-b89d-03b85872f75b-0', usage_metadata={'input_tokens': 611, 'output_tokens': 69, 'total_tokens': 680})]}}\n",
      "----\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "代理能够推断出我们查询中的“它”指的是“任务分解”，并生成了一个合理的搜索查询 - 在本例中为“任务分解的常见方法”。",
   "id": "ab98350ee1b73310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import bs4\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "### Construct retriever ###\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ],
   "id": "169d15c88e8168c4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

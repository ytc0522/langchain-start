{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangChain 作为一个框架，由许多包组成。\n",
   "id": "fa27cac4a73e2288"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## langchain-core\n",
    "此处定义了 LLM、vectorstore、检索器等核心组件的接口。此处未定义第三方集成。有意将依赖项保持为非常轻量级。"
   ],
   "id": "beb5fc97ad3b579"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 合作伙伴\n",
    "虽然集成的长尾部分在 中langchain-community，但我们将流行的集成拆分到它们自己的包中（例如langchain-openai、langchain-anthropic等）。这样做是为了改进对这些重要集成的支持。"
   ],
   "id": "c1b135879065f87d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## langchain\n",
    "主langchain包包含构成应用程序认知架构的链、代理和检索策略。这些不是第三方集成。此处的所有链、代理和检索策略都不是特定于任何一种集成的，而是所有集成的通用策略。"
   ],
   "id": "e5d85a37f35fd505"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# langchain-community\n",
    "此包包含由 LangChain 社区维护的第三方集成。主要合作伙伴包已分离出来（见下文）。它包含各种组件（LLM、vectorstore、检索器）的所有集成。此包中的所有依赖项都是可选的，以使包尽可能轻量。"
   ],
   "id": "9571cd96b259e6f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## langgraph\n",
    "langgraph是一种扩展，langchain旨在通过将步骤建模为图中的边和节点，使用 LLM 构建健壮且有状态的多参与者应用程序。\n",
    "\n",
    "LangGraph 公开了用于创建常见类型代理的高级接口，以及用于编写自定义流程的低级 API。"
   ],
   "id": "fc1e4d93e2db499e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## langserve\n",
    "一个将 LangChain 链部署为 REST API 的软件包。可轻松启动和运行可用于生产的 API。"
   ],
   "id": "68a955988f6c1d28"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"https://python.langchain.com/v0.2/svg/langchain_stack.svg\" width=\"400\" height=\"200\">\n",
   "id": "c0fafaf9a11fd5b0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## LangChain 表达语言 (LCEL）\n",
   "id": "fcedfac02188e22f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "LangChain 表达式语言 (LCEL) 是一种声明式的链接 LangChain 组件的方法。LCEL 从第一天开始就被设计为支持将原型投入生产，无需更改代码，从最简单的“prompt + LLM”链到最复杂的链（我们已经看到人们在生产中成功运行了包含 100 多个步骤的 LCEL 链）。以下是您可能想要使用 LCEL 的几个原因：",
   "id": "4f413148e2a5127"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 一流的流式支持 \n",
    "当您使用 LCEL 构建链时，您将获得最佳的第一个令牌时间（直到第一个输出块出现为止所经过的时间）。对于某些链，这意味着例如我们将令牌直接从 LLM 流式传输到流式输出解析器，然后您会以与 LLM 提供程序输出原始令牌相同的速率获得解析后的增量输出块。\n"
   ],
   "id": "bacf8dfd6104a408"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 异步支持 \n",
    "使用 LCEL 构建的任何链都可以使用同步 API（例如在原型设计时在 Jupyter 笔记本中）以及异步 API（例如在LangServe服务器中）调用。这使得原型和生产中使用相同的代码成为可能，具有出色的性能，并且能够在同一服务器中处理许多并发请求。"
   ],
   "id": "e7b4be5a1b187caa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 优化并行执行 \n",
    "每当您的 LCEL 链具有可以并行执行的步骤时（例如，如果您从多个检索器获取文档），我们都会在同步和异步接口中自动执行此操作，以尽可能减少延迟。"
   ],
   "id": "1f040d326bf65a72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 重试和回退 \n",
    "为 LCEL 链的任何部分配置重试和回退。这是让您的链在规模上更可靠的好方法。我们目前正在努力添加对重试/回退的流式支持，这样您就可以获得额外的可靠性而无需任何延迟成本。"
   ],
   "id": "b95869fafffbb806"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 访问中间结果 \n",
    "对于更复杂的链，在产生最终输出之前访问中间步骤的结果通常非常有用。这可用于让最终用户知道正在发生的事情，甚至只是调试您的链。您可以流式传输中间结果，并且它在每个LangServe服务器上都可用。"
   ],
   "id": "858439bab3526029"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 输入和输出模式 \n",
    "输入和输出模式为每个 LCEL 链提供从链结构推断出的 Pydantic 和 JSONSchema 模式。这可用于验证输入和输出，是 LangServe 不可或缺的一部分。"
   ],
   "id": "1e4ed2381d8896df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 无缝 LangSmith \n",
    "跟踪 随着您的链条变得越来越复杂，了解每一步究竟发生了什么变得越来越重要。使用 LCEL，所有步骤都会自动记录到LangSmith，以实现最大的可观察性和可调试性。"
   ],
   "id": "921969ab6d613b6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###无缝 LangServe 部署\n",
    " 使用 LCEL 创建的任何链都可以使用LangServe轻松部署。\n",
    "\n"
   ],
   "id": "985236b22de04c72"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Runnable 接口\n",
    "许多 LangChain 组件都实现了该Runnable协议，包括聊天模型、LLM、输出解析器、检索器、提示模板等"
   ],
   "id": "7aa5a410c8cb2e27"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "这是一个标准接口，可以轻松定义自定义链并以标准方式调用它们。标准接口包括：\n",
    "\n",
    "stream：流回响应块\n",
    "invoke：在输入上调用链\n",
    "batch：在输入列表上调用链"
   ],
   "id": "21e919011bc98554"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "它们还具有相应的异步方法，应与asyncio await语法一起使用以实现并发：\n",
    "\n",
    "astream：异步流回响应块\n",
    "ainvoke：在输入异步时调用链\n",
    "abatch：异步调用输入列表上的链\n",
    "astream_log：除了最终响应之外，还流回中间步骤的发生\n",
    "astream_events：链中发生的betalangchain-core流事件（在0.1.14 中引入）"
   ],
   "id": "89f98ca8f0ab3978"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The input type and output type varies by component:\n",
    "\n",
    "Component\tInput Type\tOutput Type\n",
    "Prompt\tDictionary\tPromptValue\n",
    "ChatModel\tSingle string, list of chat messages or a PromptValue\tChatMessage\n",
    "LLM\tSingle string, list of chat messages or a PromptValue\tString\n",
    "OutputParser\tThe output of an LLM or ChatModel\tDepends on the parser\n",
    "Retriever\tSingle string\tList of Documents\n",
    "Tool\tSingle string or dictionary, depending on the tool\tDepends on the tool"
   ],
   "id": "8f9fed54e788923b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 组件 Components",
   "id": "3ca429a1ba75ebbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "LangChain 为各种组件提供标准、可扩展的接口和外部集成，这些组件可用于使用 LLM 进行构建。有些组件是 LangChain 实现的，有些组件我们依赖第三方集成，还有一些则是混合的。\n",
    "\n",
    "### Chat Models\n",
    "语言模型使用一系列消息作为输入，并返回聊天消息作为输出（而不是使用纯文本）。这些传统上是较新的模型（较旧的模型通常是LLMs，见上文）。聊天模型支持为对话消息分配不同的角色，帮助区分来自人工智能、用户和系统消息等指令的消息。\n",
    "尽管底层模型是消息输入、消息输出，但 LangChain 包装器还允许这些模型将字符串作为输入。这意味着您可以轻松地使用聊天模型代替 LLM。\n",
    "\n",
    "当字符串作为输入传入时，它会转换为 HumanMessage 然后传递给底层模型。\n",
    "\n",
    "LangChain 不提供任何聊天模型，而是依赖第三方集成。\n",
    "\n",
    "在构建ChatModel时我们有一些标准化的参数：\n",
    "\n",
    "model：模型的名称\n",
    "ChatModels 还接受特定于该集成的其他参数。"
   ],
   "id": "3b6baaaaef9617b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## LLMs\n",
    "语言模型以字符串作为输入并返回字符串。这些传统上是较旧的模型（较新的模型通常是ChatModels，见下文）。\n",
    "尽管底层模型是字符串输入、字符串输出，但 LangChain 包装器还允许这些模型将消息作为输入。这使得它们可以与 ChatModel 互换。当消息作为输入传入时，它们将在后台格式化为字符串，然后再传递给底层模型。\n",
    "\n",
    "LangChain 不提供任何 LLM，而是依赖第三方集成。一些语言模型将消息列表作为输入并返回一条消息。有几种不同类型的消息。所有消息都具有role、content和response_metadata属性。"
   ],
   "id": "2c70c1dbab914c66"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 提示模板\n",
    "提示模板有助于将用户输入和参数转换为语言模型的指令。这可用于指导模型的响应，帮助其理解上下文并生成相关且连贯的基于语言的输出。\n",
    "\n",
    "提示模板以字典作为输入，其中每个键代表提示模板中要填写的变量。\n",
    "\n",
    "Prompt Templates 输出 PromptValue。此 PromptValue 可以传递给 LLM 或 ChatModel，也可以转换为字符串或消息列表。此 PromptValue 存在的原因是为了便于在字符串和消息之间切换。\n",
    "\n",
    "有几种不同类型的提示模板"
   ],
   "id": "b9c5b72d1c77a4bc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 字符串\n",
    "这些提示模板用于格式化单个字符串，通常用于较简单的输入。例如，构造和使用 PromptTemplate 的常见方法如下："
   ],
   "id": "129b7b385dc07f66"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:34:29.609640Z",
     "start_time": "2024-06-07T07:34:29.605980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "\n",
    "value = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(type(value))\n"
   ],
   "id": "4791e87c621c433e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:36:15.789347Z",
     "start_time": "2024-06-07T07:36:15.784662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant for {topic}\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ],
   "id": "6f841fac565566df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant for cats'), HumanMessage(content='Tell me a joke about cats')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:38:49.184067Z",
     "start_time": "2024-06-07T07:38:49.178222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})"
   ],
   "id": "d40cabfd9b56559f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='hi!')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])"
   ],
   "id": "7fe9196f268d848f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 输出解析器\n",
    "此处的信息指的是解析器从模型中获取文本输出，并尝试将其解析为更结构化的表示。越来越多的模型支持函数（或工具）调用，可自动处理此问题。建议使用函数/工具调用而不是输出解析。\n",
    "\n",
    "负责获取模型的输出并将其转换为更适合下游任务的格式。当您使用 LLM 生成结构化数据或规范化聊天模型和 LLM 的输出时很有用。\n",
    "\n",
    "LangChain 有许多不同类型的输出解析器"
   ],
   "id": "f925c2e93593cb93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 聊天记录\n",
    "ChatHistory是指 LangChain 中的一个类，可用于包装任意链。这ChatHistory将跟踪底层链的输入和输出，并将它们作为消息附加到消息数据库中。未来的交互将加载这些消息并将它们作为输入的一部分传递到链中。"
   ],
   "id": "aa790a50ef71b1aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 文档加载器\n",
    "这些类加载Document对象。LangChain 与各种数据源有数百个集成，可从以下数据源加载数据：Slack、Notion、Google Drive 等。\n",
    "\n",
    "每个 DocumentLoader 都有自己特定的参数，但它们都可以用相同的方法调用.load。示例用例如下：\n"
   ],
   "id": "2497e74b911b88d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(\n",
    "    ...  # <-- Integration specific parameters here\n",
    ")\n",
    "data = loader.load()"
   ],
   "id": "934598f4c2aa26ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 文本分割器\n",
    "加载文档后，您经常需要对它们进行转换，以更好地适应您的应用程序。最简单的例子是，您可能希望将较长的文档拆分为较小的块，以便放入模型的上下文窗口。LangChain 有许多内置的文档转换器，可轻松拆分、合并、过滤和以其他方式操作文档。\n",
    "\n",
    "当您想要处理长文本时，需要将文本拆分成块。虽然这听起来很简单，但这里却存在很多潜在的复杂性。理想情况下，您希望将语义相关的文本片段放在一起。“语义相关”的含义可能取决于文本的类型。本笔记本展示了几种实现此目的的方法。"
   ],
   "id": "aa9e6617a59a116a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "从高层次来看，文本分割器的工作原理如下：\n",
    "\n",
    "将文本分成小的、语义上有意义的块（通常是句子）。\n",
    "开始将这些小块组合成更大的块，直到达到一定大小（由某些函数衡量）。\n",
    "一旦达到该大小，就将该块作为其自己的文本，然后开始创建具有一些重叠的新文本块（以保留块之间的上下文）。\n",
    "这意味着您可以沿着两个不同的轴自定义文本分割器：\n",
    "\n",
    "文本如何分割\n",
    "如何测量块大小"
   ],
   "id": "ef88ffb40570daa8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 嵌入模型\n",
    "Embeddings 类是专为与文本嵌入模型交互而设计的类。有许多嵌入模型提供程序（OpenAI、Cohere、Hugging Face 等） - 此类旨在为所有这些提供程序提供标准接口。\n",
    "\n",
    "嵌入会创建一段文本的向量表示。这很有用，因为这意味着我们可以在向量空间中思考文本，并执行语义搜索等操作，即在向量空间中寻找最相似的文本片段。\n",
    "\n",
    "LangChain 中的基础 Embeddings 类提供了两种方法：一种用于嵌入文档，一种用于嵌入查询。前者接受多个文本作为输入，而后者接受单个文本。将它们作为两个独立方法的原因是，某些嵌入提供程序对文档（要搜索的文档）和查询（搜索查询本身）有不同的嵌入方法。\n",
    "\n"
   ],
   "id": "1fdd03f6e5a112b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 向量存储\n",
    "存储和搜索非结构化数据的最常见方法之一是嵌入数据并存储生成的嵌入向量，然后在查询时嵌入非结构化查询并检索与嵌入查询“最相似”的嵌入向量。向量存储负责存储嵌入数据并为您执行向量搜索。\n",
    "\n",
    "可以通过执行以下操作将向量存储转换为检索器接口："
   ],
   "id": "cba28935f7f7b4cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "vectorstore = MyVectorStore()\n",
    "retriever = vectorstore.as_retriever()"
   ],
   "id": "692843e8f201ebe3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 检索器\n",
    "检索器是一种接口，可根据非结构化查询返回文档。它比向量存储更通用。检索器不需要能够存储文档，只需返回（或检索）文档即可。检索器可以从向量存储中创建，但也足够广泛。检索器接受字符串查询作为输入并返回文档列表作为输出。"
   ],
   "id": "1b3215b077cce25c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 工具\n",
    "工具是代理、链或聊天模型/LLM 可以用来与世界互动的接口。\n",
    "工具由以下组件组成：\n",
    "\n",
    "工具名称\n",
    "该工具的功能描述\n",
    "该工具输入的 JSON 模式\n",
    "要调用的函数\n",
    "工具的结果是否应直接返回给用户（仅与代理相关）"
   ],
   "id": "b359f66a0410b294"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "名称、描述和 JSON 模式作为 LLM 的上下文提供，从而允许 LLM 确定如何适当地使用该工具。\n",
    "\n",
    "给定可用工具列表和提示，LLM 可以请求使用适当的参数调用一个或多个工具。\n",
    "\n",
    "一般来说，在设计聊天模型或 LLM 使用的工具时，需要牢记以下几点：\n",
    "\n",
    "针对工具调用进行过微调的聊天模型在工具调用方面会比未经微调的模型表现得更好。\n",
    "未经微调的模型可能根本无法使用工具，尤其是在工具很复杂或需要多次工具调用的情况下。\n",
    "如果工具具有精心选择的名称、描述和 JSON 模式，模型的性能将更好。\n",
    "与复杂的工具相比，简单的工具通常更容易被模型使用。"
   ],
   "id": "dfc59ef84d3b4cb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 工具包\n",
    "是一组工具的集合，旨在一起用于特定任务。它们具有方便的加载方法。\n",
    "\n",
    "所有工具包都公开一个get_tools返回工具列表的方法。因此您可以执行以下操作：\n",
    "\n",
    "# Initialize a toolkit\n",
    "toolkit = ExampleTookit(...)\n",
    "\n",
    "# Get list of tools\n",
    "tools = toolkit.get_tools()"
   ],
   "id": "c6258da26421a8d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
